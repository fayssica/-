{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_data.csv')\n",
    "test = pd.read_csv('./data/test_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 初步清洗 0.87410 为了节省空间只保留一个测试模型代码，后面都使用相同参数\n",
    "target = 'tradeMoney'\n",
    "test[target] = -1\n",
    "data = pd.concat([train,test])\n",
    "\n",
    "columns = test.columns.tolist()\n",
    "columns.remove(target)\n",
    "columns.remove(\"ID\")\n",
    "object_col = ['buildYear','city','communityName','houseDecoration','houseFloor','houseToward','houseType',\n",
    "             'plate','region','rentType','tradeTime'] # object型特征\n",
    "num_col = [x for x in columns if x not in object_col] # 数值型特征\n",
    "\n",
    "# 缺失值处理\n",
    "data['pv'] = data['pv'].fillna(data['pv'].mean())\n",
    "data['uv'] = data['uv'].fillna(data['uv'].mean())\n",
    "\n",
    "median_year = data[data['buildYear'] != '暂无信息']['buildYear'].median()\n",
    "data['buildYear'][data['buildYear'] == '暂无信息'] = median_year\n",
    "data['buildYear'] = data['buildYear'].astype(int)\n",
    "object_col.remove('buildYear')\n",
    "columns.remove('houseDecoration')\n",
    "object_col.remove('houseDecoration')\n",
    "data['houseToward'][data['houseToward']=='暂无数据'] = '南'\n",
    "\n",
    "# 异常值处理\n",
    "data.drop(data[(data[target]>50000)].index,inplace=True) \n",
    "data.drop(data[data['houseType']=='0室0厅1卫'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1620]\ttraining's rmse: 726.934\tvalid_1's rmse: 1443.18\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's rmse: 742.211\tvalid_1's rmse: 1250.52\n",
      "Early stopping, best iteration is:\n",
      "[2426]\ttraining's rmse: 691.525\tvalid_1's rmse: 1249.43\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1782]\ttraining's rmse: 745.24\tvalid_1's rmse: 1450.89\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's rmse: 730.869\tvalid_1's rmse: 1273.37\n",
      "Early stopping, best iteration is:\n",
      "[2039]\ttraining's rmse: 725.527\tvalid_1's rmse: 1273.01\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's rmse: 728.229\tvalid_1's rmse: 1316.78\n",
      "Early stopping, best iteration is:\n",
      "[2067]\ttraining's rmse: 719.951\tvalid_1's rmse: 1316.42\n",
      "CV Score: 0.87410 \n",
      "CPU times: user 4min 29s, sys: 3.03 s, total: 4min 32s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'min_child_samples':20,\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_seed\": 23,\n",
    "    \"metric\": 'rmse',\n",
    "    \"lambda_l1\": 0.2,\n",
    "    \"nthread\": 4,\n",
    "}\n",
    "\n",
    "train = data[data[target] != -1][columns]\n",
    "test = data[data[target] == -1][columns]\n",
    "tar = data[data[target] != -1][target]\n",
    "\n",
    "for col in object_col:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "    \n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2333)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx], \n",
    "                           label=tar.iloc[trn_idx],\n",
    "                           categorical_feature=object_col)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx], \n",
    "                           label=tar.iloc[val_idx],\n",
    "                           categorical_feature=object_col)\n",
    "\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=2000, early_stopping_rounds = 200)\n",
    "    \n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "print(\"CV Score: {:<8.5f}\".format(r2_score(tar, oof_lgb))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['area', 'rentType', 'houseFloor', 'totalFloor', 'houseToward', 'communityName', 'region', 'plate', 'buildYear', 'saleSecHouseNum', 'subwayStationNum', 'busStationNum', 'interSchoolNum', 'schoolNum', 'privateSchoolNum', 'hospitalNum', 'drugStoreNum', 'gymNum', 'bankNum', 'shopNum', 'parkNum', 'mallNum', 'superMarketNum', 'totalTradeMoney', 'totalTradeArea', 'tradeMeanPrice', 'tradeSecNum', 'totalNewTradeMoney', 'totalNewTradeArea', 'tradeNewMeanPrice', 'tradeNewNum', 'remainNewNum', 'supplyNewNum', 'supplyLandNum', 'supplyLandArea', 'tradeLandNum', 'tradeLandArea', 'landTotalPrice', 'landMeanPrice', 'totalWorkers', 'newWorkers', 'residentPopulation', 'pv', 'uv', 'lookNum', 'room', 'living', 'toilet', 'tradeMonth', 'tradeDate']\n",
      "['communityName', 'houseFloor', 'houseToward', 'plate', 'region', 'rentType']\n"
     ]
    }
   ],
   "source": [
    "# 分割特征 0.88120\n",
    "train = train[train['area'] < 200]\n",
    "train = train[train['area'] > 10]\n",
    "\n",
    "data['room'] = data['houseType'].apply(lambda x : x.split('室')[0][-1]).astype(int)\n",
    "data['living'] = data['houseType'].apply(lambda x : x.split('厅')[0][-1]).astype(int)\n",
    "data['toilet'] = data['houseType'].apply(lambda x : x.split('卫')[0][-1]).astype(int)\n",
    "columns.remove('houseType')\n",
    "object_col.remove('houseType')\n",
    "columns.extend(['room','living','toilet'])\n",
    "data['tradeYear'] = data['tradeTime'].apply(lambda x : x.split('/')[0]).astype(int)\n",
    "data['tradeMonth'] = data['tradeTime'].apply(lambda x : x.split('/')[1]).astype(int)\n",
    "data['tradeDate'] = data['tradeTime'].apply(lambda x : x.split('/')[2]).astype(int)\n",
    "columns.remove('tradeTime')\n",
    "object_col.remove('tradeTime')\n",
    "columns.extend(['tradeYear','tradeMonth','tradeDate'])\n",
    "columns.remove('city')\n",
    "object_col.remove('city')\n",
    "columns.remove('tradeYear')\n",
    "print(columns)\n",
    "print(object_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 合并特征  0.88099\n",
    "data['numRooms'] = data['room']+data['living']+data['toilet']\n",
    "data['numTansportEquipment'] = data['subwayStationNum']+data['busStationNum']\n",
    "data['numMedical'] = data['hospitalNum']+data['drugStoreNum']\n",
    "data['numEducation'] = data['interSchoolNum']+data['schoolNum']+data['privateSchoolNum']\n",
    "data['numLiving'] = data['gymNum']+data['parkNum']+data['bankNum']\n",
    "data['numShop'] = data['shopNum']+data['mallNum']+data['superMarketNum']\n",
    "columns.extend(['numRooms','numTansportEquipment','numMedical','numEducation','numLiving','numShop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 根据房间数，所在楼层，朝向进行组合  0.88163 \n",
    "gp = data.groupby('numRooms')['houseToward'].value_counts().rename('numRooms_houseToward_count',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on=['numRooms','houseToward'])\n",
    "gp = data.groupby('numRooms')['houseFloor'].value_counts().rename('numRooms_houseFloor_count',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on=['numRooms','houseFloor'])\n",
    "gp = data.groupby('houseToward')['houseFloor'].value_counts().rename('houseToward_houseFloor_count',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on=['houseToward','houseFloor'])\n",
    "columns.extend(['numRooms_houseToward_count','numRooms_houseFloor_count','houseToward_houseFloor_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 根据区域groupby 0.88168 \n",
    "gp = data.groupby('region')['numRooms'].mean().rename('region_numRooms_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numTansportEquipment'].mean().rename('region_numTansportEquipment_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numMedical'].mean().rename('region_numMedical_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numEducation'].mean().rename('region_numEducation_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numLiving'].mean().rename('region_numLiving_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numShop'].mean().rename('region_numShop_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['area'].mean().rename('region_area_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['area'].std().rename('region_area_std',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['area'].median().rename('region_area_median',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['houseToward'].value_counts().rename('region_houseToward_count',inplace=True)\n",
    "data = pd.merge(data, gp, how='left', on=['region','houseToward'])\n",
    "columns.extend(['region_numRooms_mean','region_numTansportEquipment_mean','region_numMedical_mean','region_numEducation_mean',\n",
    "               'region_numLiving_mean','region_numShop_mean','region_houseToward_count','region_area_mean','region_area_std',\n",
    "               'region_area_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 根据小区信息groupby 0.88933 \n",
    "gp = data.groupby('communityName')['numRooms'].mean().rename('communityName_numRooms_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numTansportEquipment'].mean().rename('communityName_numTansportEquipment_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numMedical'].mean().rename('communityName_numMedical_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numEducation'].mean().rename('communityName_numEducation_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numLiving'].mean().rename('communityName_numLiving_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numShop'].mean().rename('communityName_numShop_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['area'].mean().rename('communityName_area_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['area'].std().rename('communityName_area_std',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['area'].median().rename('communityName_area_median',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "\n",
    "gp = data.groupby('communityName')['houseToward'].value_counts().rename('communityName_houseToward_count',inplace=True)\n",
    "data = pd.merge(data, gp, how='left', on=['communityName','houseToward'])\n",
    "columns.extend(['communityName_numRooms_mean','communityName_numTansportEquipment_mean','communityName_numMedical_mean',\n",
    "                'communityName_numEducation_mean','communityName_numLiving_mean','communityName_numShop_mean',\n",
    "                'communityName_area_mean','communityName_area_std','communityName_area_median','communityName_houseToward_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Word2vec特征 0.89278 \n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "save_path = './w2v' \n",
    "\n",
    "L = 10\n",
    "sentence = []\n",
    "for line in list(data[['communityName', 'plate', 'region', 'houseToward']].values):\n",
    "    sentence.append([str(l) for idx, l in enumerate(line)])\n",
    "    \n",
    "model = Word2Vec(sentence, size=L, window=2, min_count=1, workers=multiprocessing.cpu_count(),iter=10)\n",
    "for fea in ['communityName', 'plate', 'region', 'houseToward']:\n",
    "    values = []\n",
    "    for line in list(data[fea].values):\n",
    "        values.append(line)\n",
    "    values = set(values)\n",
    "    w2v = []\n",
    "    for i in values:\n",
    "        a = [i]\n",
    "        a.extend(model[str(i)])\n",
    "        w2v.append(a)\n",
    "    out_df = pd.DataFrame(w2v)\n",
    "\n",
    "    name = [fea]\n",
    "    for i in range(L):\n",
    "        name.append(name[0] + 'W' + str(i))\n",
    "    out_df.columns = name\n",
    "    out_df.to_csv(save_path + '/' + fea + '.csv', index=False)\n",
    "\n",
    "w2v_features = []\n",
    "for col in ['communityName', 'plate', 'region', 'houseToward']:\n",
    "    df = pd.read_csv(save_path + '/' + col + '.csv')\n",
    "    df = df.drop_duplicates([col])\n",
    "    fs = list(df)\n",
    "    fs.remove(col)\n",
    "    w2v_features += fs\n",
    "    data = pd.merge(data, df, on=col, how='left')\n",
    "columns.extend(w2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lebel编码  0.89278 对lgb来说指定了categorical feature就相当于编码了\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for i in object_col:\n",
    "    lbl = LabelEncoder()\n",
    "    data[i] = lbl.fit_transform(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spherical\n",
      "tied\n",
      "diag\n",
      "full\n"
     ]
    }
   ],
   "source": [
    "# 聚类特征  0.89295 提升一点点不知道这么用对不对\n",
    "from sklearn.mixture import GaussianMixture  \n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "res = []\n",
    "for cv_type in cv_types:\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type=cv_type)\n",
    "    gmm.fit(data[columns].fillna(0))\n",
    "    res.append(gmm.predict(data[columns].fillna(0)))\n",
    "gm_df = pd.DataFrame(res).T\n",
    "gm_df.rename(columns={0:'gm_spherical',1:'gm_tied',2:'gm_diag',3:'gm_full'},inplace=True)\n",
    "data = pd.concat([data,gm_df],axis=1)\n",
    "columns.extend(gm_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前有对特征选择做过一些研究，推荐一下Jungdong Li的论文Feature Selection: A Data Perspective，他做了一个库https://github.com/jundongl/scikit-feature 以及他的网站http://featureselection.asu.edu (里面有数据集和算法说明) py3安装时可能需要根据报错提示做一些源码修改，后面也有不少bug，不过聊胜于无。 这里随便挑了几个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.statistical_based import f_score\n",
    "from skfeature.function.statistical_based import chi_square\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data['tradeMoney'] != -1]\n",
    "test = data[data['tradeMoney'] == -1][columns]\n",
    "train_Y = train['tradeMoney']\n",
    "train_X = train[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_area_std\n",
      "communityName_area_std\n"
     ]
    }
   ],
   "source": [
    "# 存在空值的特征\n",
    "for i in train_X.columns:\n",
    "    if train_X[i].isnull().sum() > 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 60.1 ms, total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RF\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor()\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "rf_w = clf.feature_importances_\n",
    "sort_idx = np.argsort(rf_w)\n",
    "feature_rank_RF = np.array(columns)[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 51s, sys: 1.35 s, total: 19min 52s\n",
      "Wall time: 19min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# RFE默认只用了一个核，跑的时间会长一些\n",
    "estimator = RandomForestRegressor()\n",
    "selector = RFE(estimator, 1, step=1)\n",
    "selector = selector.fit(train_X, train_Y)\n",
    "RFE_w = selector.ranking_\n",
    "feature_rank_RFE = np.array(columns)[RFE_w-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 258 ms, sys: 40.3 ms, total: 299 ms\n",
      "Wall time: 296 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# f_score\n",
    "f_w = f_score.f_score(train_X,train_Y)\n",
    "sort_idx = np.argsort(f_w)\n",
    "feature_rank_fscore = np.array(columns)[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f204bdfe7ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#选择k个最佳特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mchi2_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "# 卡方检验 报了个错，X非正定\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "model1 = SelectKBest(chi2, k='all')#选择k个最佳特征\n",
    "model1.fit_transform(train_X, train_Y)\n",
    "chi2_w = model1.scores_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[393]\ttraining's rmse: 3050.91\tvalid_1's rmse: 3081.6\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[382]\ttraining's rmse: 3042.09\tvalid_1's rmse: 3125.06\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[629]\ttraining's rmse: 3006.47\tvalid_1's rmse: 3192.35\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[407]\ttraining's rmse: 3059.12\tvalid_1's rmse: 3039.78\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[370]\ttraining's rmse: 3047.98\tvalid_1's rmse: 3098.05\n",
      "CV Score: 0.33206 \n",
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1577]\ttraining's rmse: 858.759\tvalid_1's rmse: 1485\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5757]\ttraining's rmse: 655.294\tvalid_1's rmse: 1245.47\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2784]\ttraining's rmse: 782.922\tvalid_1's rmse: 1430.83\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4341]\ttraining's rmse: 709.396\tvalid_1's rmse: 1224.82\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1911]\ttraining's rmse: 877.725\tvalid_1's rmse: 1353.92\n",
      "CV Score: 0.87362 \n",
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5890]\ttraining's rmse: 1257.91\tvalid_1's rmse: 2202.54\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7236]\ttraining's rmse: 1185.43\tvalid_1's rmse: 2078.3\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5370]\ttraining's rmse: 1286.86\tvalid_1's rmse: 2263.24\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9670]\ttraining's rmse: 1080.66\tvalid_1's rmse: 2025.06\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8829]\ttraining's rmse: 1102.41\tvalid_1's rmse: 2205.76\n",
      "CV Score: 0.67830 \n"
     ]
    }
   ],
   "source": [
    "'''取排名前50的特征进行训练，比较结果'''\n",
    "# RF CV Score: 0.33206\n",
    "# RFE CV Score: 0.87362\n",
    "# f_score CV Score: 0.67830 \n",
    "select_num = 50\n",
    "rank_list = [feature_rank_RF,feature_rank_RFE,feature_rank_fscore]\n",
    "for i in rank_list:\n",
    "    use_cols = i[:select_num]\n",
    "    params = {\n",
    "        'num_leaves': 31,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'min_child_samples':20,\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"bagging_fraction\": 0.85,\n",
    "        \"bagging_seed\": 23,\n",
    "        \"metric\": 'rmse',\n",
    "        \"lambda_l1\": 0.2,\n",
    "        \"nthread\": 4,\n",
    "    }\n",
    "\n",
    "    train = data[data['tradeMoney'] != -1][use_cols]\n",
    "    test = data[data['tradeMoney'] == -1][use_cols]\n",
    "    tar = data[data['tradeMoney'] != -1]['tradeMoney']\n",
    "    cat_cols = [x for x in use_cols if x in object_col]\n",
    "    for col in cat_cols:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2333)\n",
    "    oof_lgb = np.zeros(len(train))\n",
    "    predictions_lgb = np.zeros(len(test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values)):\n",
    "        print(\"fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx], \n",
    "                               label=tar.iloc[trn_idx],\n",
    "                               categorical_feature=cat_cols)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx], \n",
    "                               label=tar.iloc[val_idx],\n",
    "                               categorical_feature=cat_cols)\n",
    "\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=10000, early_stopping_rounds = 200)\n",
    "\n",
    "        oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = use_cols\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print(\"CV Score: {:<8.5f}\".format(r2_score(tar, oof_lgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[10000]\ttraining's rmse: 622.566\tvalid_1's rmse: 1651.07\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's rmse: 622.566\tvalid_1's rmse: 1651.07\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7591]\ttraining's rmse: 739.806\tvalid_1's rmse: 1444.66\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4210]\ttraining's rmse: 865.076\tvalid_1's rmse: 1710.15\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6890]\ttraining's rmse: 762.482\tvalid_1's rmse: 1361.59\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8292]\ttraining's rmse: 713.44\tvalid_1's rmse: 1430.75\n",
      "CV Score: 0.83902 \n",
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1846]\ttraining's rmse: 729.962\tvalid_1's rmse: 1357.72\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2929]\ttraining's rmse: 719.115\tvalid_1's rmse: 1141.24\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1971]\ttraining's rmse: 766.992\tvalid_1's rmse: 1327.73\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4570]\ttraining's rmse: 628.196\tvalid_1's rmse: 1127.03\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1537]\ttraining's rmse: 827.104\tvalid_1's rmse: 1242.14\n",
      "CV Score: 0.89320 \n",
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5787]\ttraining's rmse: 1175.68\tvalid_1's rmse: 2105.13\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5696]\ttraining's rmse: 1197.52\tvalid_1's rmse: 1977.13\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4216]\ttraining's rmse: 1286.87\tvalid_1's rmse: 2152.84\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5929]\ttraining's rmse: 1177.75\tvalid_1's rmse: 1950.92\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5224]\ttraining's rmse: 1213.85\tvalid_1's rmse: 2105.43\n",
      "CV Score: 0.70658 \n"
     ]
    }
   ],
   "source": [
    "'''取排名前100的特征进行训练，比较结果'''\n",
    "# RF CV Score: 0.83902\n",
    "# RFE CV Score: 0.89320\n",
    "# f_score CV Score: 0.70658\n",
    "select_num = 100\n",
    "rank_list = [feature_rank_RF,feature_rank_RFE,feature_rank_fscore]\n",
    "for i in rank_list:\n",
    "    use_cols = i[:select_num]\n",
    "    params = {\n",
    "        'num_leaves': 31,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'min_child_samples':20,\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.01,\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"bagging_fraction\": 0.85,\n",
    "        \"bagging_seed\": 23,\n",
    "        \"metric\": 'rmse',\n",
    "        \"lambda_l1\": 0.2,\n",
    "        \"nthread\": 4,\n",
    "    }\n",
    "\n",
    "    train = data[data['tradeMoney'] != -1][use_cols]\n",
    "    test = data[data['tradeMoney'] == -1][use_cols]\n",
    "    tar = data[data['tradeMoney'] != -1]['tradeMoney']\n",
    "    cat_cols = [x for x in use_cols if x in object_col]\n",
    "    for col in cat_cols:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=2333)\n",
    "    oof_lgb = np.zeros(len(train))\n",
    "    predictions_lgb = np.zeros(len(test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values)):\n",
    "        print(\"fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx], \n",
    "                               label=tar.iloc[trn_idx],\n",
    "                               categorical_feature=cat_cols)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx], \n",
    "                               label=tar.iloc[val_idx],\n",
    "                               categorical_feature=cat_cols)\n",
    "\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=10000, early_stopping_rounds = 200)\n",
    "\n",
    "        oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = use_cols\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    print(\"CV Score: {:<8.5f}\".format(r2_score(tar, oof_lgb)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
