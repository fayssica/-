{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2469, 50)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train_data.csv')\n",
    "test = pd.read_csv('./data/test_a.csv')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 初步清洗 0.87410 r2 0.9505601666837905为了节省空间只保留一个测试模型代码，后面都使用相同参数\n",
    "target = 'tradeMoney'\n",
    "test[target] = -1\n",
    "data = pd.concat([train,test])\n",
    "\n",
    "columns = test.columns.tolist()\n",
    "columns.remove(target)\n",
    "columns.remove(\"ID\")\n",
    "object_col = ['buildYear','city','communityName','houseDecoration','houseFloor','houseToward','houseType',\n",
    "             'plate','region','rentType','tradeTime'] # object型特征\n",
    "num_col = [x for x in columns if x not in object_col] # 数值型特征\n",
    "\n",
    "# 缺失值处理\n",
    "data['pv'] = data['pv'].fillna(data['pv'].mean())\n",
    "data['uv'] = data['uv'].fillna(data['uv'].mean())\n",
    "\n",
    "median_year = data[data['buildYear'] != '暂无信息']['buildYear'].median()\n",
    "data['buildYear'][data['buildYear'] == '暂无信息'] = median_year\n",
    "data['buildYear'] = data['buildYear'].astype(int)\n",
    "object_col.remove('buildYear')\n",
    "columns.remove('houseDecoration')\n",
    "object_col.remove('houseDecoration')\n",
    "data['houseToward'][data['houseToward']=='暂无数据'] = '南'\n",
    "\n",
    "# 异常值处理\n",
    "train = data[data[target] != -1]\n",
    "test = data[data[target] == -1]\n",
    "\n",
    "train.drop(train[(train[target]>50000)].index,inplace=True) \n",
    "train.drop(train[train['houseType']=='0室0厅1卫'].index,inplace=True)\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 分割特征 0.88120 r2 0.9523926133806397\n",
    "train = train[train['area'] < 200]\n",
    "train = train[train['area'] > 10]\n",
    "\n",
    "data['room'] = data['houseType'].apply(lambda x : x.split('室')[0][-1]).astype(int)\n",
    "data['living'] = data['houseType'].apply(lambda x : x.split('厅')[0][-1]).astype(int)\n",
    "data['toilet'] = data['houseType'].apply(lambda x : x.split('卫')[0][-1]).astype(int)\n",
    "columns.remove('houseType')\n",
    "object_col.remove('houseType')\n",
    "columns.extend(['room','living','toilet'])\n",
    "data['tradeYear'] = data['tradeTime'].apply(lambda x : x.split('/')[0]).astype(int)\n",
    "data['tradeMonth'] = data['tradeTime'].apply(lambda x : x.split('/')[1]).astype(int)\n",
    "data['tradeDate'] = data['tradeTime'].apply(lambda x : x.split('/')[2]).astype(int)\n",
    "columns.remove('tradeTime')\n",
    "object_col.remove('tradeTime')\n",
    "columns.extend(['tradeYear','tradeMonth','tradeDate'])\n",
    "columns.remove('city')\n",
    "object_col.remove('city')\n",
    "columns.remove('tradeYear')\n",
    "# print(columns)\n",
    "# print(object_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # 合并特征  \n",
    "data['numRooms'] = data['room']+data['living']+data['toilet']\n",
    "data['numTansportEquipment'] = data['subwayStationNum']+data['busStationNum']\n",
    "data['numMedical'] = data['hospitalNum']+data['drugStoreNum']\n",
    "data['numEducation'] = data['interSchoolNum']+data['schoolNum']+data['privateSchoolNum']\n",
    "data['numLiving'] = data['gymNum']+data['parkNum']+data['bankNum']\n",
    "data['numShop'] = data['shopNum']+data['mallNum']+data['superMarketNum']\n",
    "# columns.extend(['numRooms','numTansportEquipment','numMedical','numEducation','numLiving','numShop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # 根据房间数，所在楼层，朝向进行组合  0.88163  r2  0.9505506582265562\n",
    "# gp = data.groupby('numRooms')['houseToward'].value_counts().rename('numRooms_houseToward_count',inplace=True)     \n",
    "# data = pd.merge(data, gp, how='left', on=['numRooms','houseToward'])\n",
    "# gp = data.groupby('numRooms')['houseFloor'].value_counts().rename('numRooms_houseFloor_count',inplace=True)     \n",
    "# data = pd.merge(data, gp, how='left', on=['numRooms','houseFloor'])\n",
    "# gp = data.groupby('houseToward')['houseFloor'].value_counts().rename('houseToward_houseFloor_count',inplace=True)     \n",
    "# data = pd.merge(data, gp, how='left', on=['houseToward','houseFloor'])\n",
    "# columns.extend(['numRooms_houseToward_count','numRooms_houseFloor_count','houseToward_houseFloor_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 根据区域groupby 0.88168  r2 0.9503471924087608 -> 0.952271276439161\n",
    "gp = data.groupby('region')['numRooms'].mean().rename('region_numRooms_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numTansportEquipment'].mean().rename('region_numTansportEquipment_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numMedical'].mean().rename('region_numMedical_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numEducation'].mean().rename('region_numEducation_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numLiving'].mean().rename('region_numLiving_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['numShop'].mean().rename('region_numShop_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['area'].mean().rename('region_area_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['area'].std().rename('region_area_std',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['area'].median().rename('region_area_median',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='region')\n",
    "gp = data.groupby('region')['houseToward'].value_counts().rename('region_houseToward_count',inplace=True)\n",
    "data = pd.merge(data, gp, how='left', on=['region','houseToward'])\n",
    "columns.extend(['region_numRooms_mean','region_numTansportEquipment_mean','region_numMedical_mean','region_numEducation_mean',\n",
    "               'region_numLiving_mean','region_numShop_mean','region_houseToward_count','region_area_mean','region_area_std',\n",
    "               'region_area_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 根据小区信息groupby 0.88933  r2 0.9524753772367441 -> 0.9528857144503925\n",
    "gp = data.groupby('communityName')['numRooms'].mean().rename('communityName_numRooms_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numTansportEquipment'].mean().rename('communityName_numTansportEquipment_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numMedical'].mean().rename('communityName_numMedical_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numEducation'].mean().rename('communityName_numEducation_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numLiving'].mean().rename('communityName_numLiving_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['numShop'].mean().rename('communityName_numShop_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['area'].mean().rename('communityName_area_mean',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['area'].std().rename('communityName_area_std',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "gp = data.groupby('communityName')['area'].median().rename('communityName_area_median',inplace=True)     \n",
    "data = pd.merge(data, gp, how='left', on='communityName')\n",
    "\n",
    "gp = data.groupby('communityName')['houseToward'].value_counts().rename('communityName_houseToward_count',inplace=True)\n",
    "data = pd.merge(data, gp, how='left', on=['communityName','houseToward'])\n",
    "columns.extend(['communityName_numRooms_mean','communityName_numTansportEquipment_mean','communityName_numMedical_mean',\n",
    "                'communityName_numEducation_mean','communityName_numLiving_mean','communityName_numShop_mean',\n",
    "                'communityName_area_mean','communityName_area_std','communityName_area_median','communityName_houseToward_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Word2vec特征 0.89278 r2 0.9550027597748323 -> 0.9573779127898738  \n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "save_path = './w2v' \n",
    "\n",
    "L = 10\n",
    "sentence = []\n",
    "for line in list(data[['communityName', 'plate', 'region', 'houseToward']].values):\n",
    "    sentence.append([str(l) for idx, l in enumerate(line)])\n",
    "    \n",
    "model = Word2Vec(sentence, size=L, window=2, min_count=1, workers=multiprocessing.cpu_count(),iter=10)\n",
    "for fea in ['communityName', 'plate', 'region', 'houseToward']:\n",
    "    values = []\n",
    "    for line in list(data[fea].values):\n",
    "        values.append(line)\n",
    "    values = set(values)\n",
    "    w2v = []\n",
    "    for i in values:\n",
    "        a = [i]\n",
    "        a.extend(model[str(i)])\n",
    "        w2v.append(a)\n",
    "    out_df = pd.DataFrame(w2v)\n",
    "\n",
    "    name = [fea]\n",
    "    for i in range(L):\n",
    "        name.append(name[0] + 'W' + str(i))\n",
    "    out_df.columns = name\n",
    "    out_df.to_csv(save_path + '/' + fea + '.csv', index=False)\n",
    "\n",
    "w2v_features = []\n",
    "for col in ['communityName', 'plate', 'region', 'houseToward']:\n",
    "    df = pd.read_csv(save_path + '/' + col + '.csv')\n",
    "    df = df.drop_duplicates([col])\n",
    "    fs = list(df)\n",
    "    fs.remove(col)\n",
    "    w2v_features += fs\n",
    "    data = pd.merge(data, df, on=col, how='left')\n",
    "columns.extend(w2v_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 聚类特征  0.89295 r2 0.9555599679001301 -> 0.9561665920565924 提升一点点不知道这么用对不对\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for i in object_col:\n",
    "    lbl = LabelEncoder()\n",
    "    data[i] = lbl.fit_transform(data[i])\n",
    "    \n",
    "from sklearn.mixture import GaussianMixture  \n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "res = []\n",
    "for cv_type in cv_types:\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type=cv_type)\n",
    "    gmm.fit(data[columns].fillna(0))\n",
    "    res.append(gmm.predict(data[columns].fillna(0)))\n",
    "gm_df = pd.DataFrame(res).T\n",
    "gm_df.rename(columns={0:'gm_spherical',1:'gm_tied',2:'gm_diag',3:'gm_full'},inplace=True)\n",
    "data = pd.concat([data,gm_df],axis=1)\n",
    "columns.extend(gm_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1409]\ttraining's rmse: 783.083\tvalid_1's rmse: 1386.1\n",
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's rmse: 795.757\tvalid_1's rmse: 1173.67\n",
      "Early stopping, best iteration is:\n",
      "[2790]\ttraining's rmse: 732.228\tvalid_1's rmse: 1168.94\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's rmse: 764.426\tvalid_1's rmse: 1340.26\n",
      "Early stopping, best iteration is:\n",
      "[2216]\ttraining's rmse: 746.356\tvalid_1's rmse: 1339.72\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's rmse: 787.021\tvalid_1's rmse: 1145.89\n",
      "Early stopping, best iteration is:\n",
      "[3464]\ttraining's rmse: 684.332\tvalid_1's rmse: 1136.68\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1435]\ttraining's rmse: 846.07\tvalid_1's rmse: 1263.08\n",
      "CV Score: 0.88976 \n",
      "线上R^2 score: 0.9561629507148155\n",
      "CPU times: user 10min 40s, sys: 5.57 s, total: 10min 45s\n",
      "Wall time: 10min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'min_child_samples':20,\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_seed\": 23,\n",
    "    \"metric\": 'rmse',\n",
    "    \"lambda_l1\": 0.2,\n",
    "    \"nthread\": 1,\n",
    "}\n",
    "\n",
    "train = data[data[target] != -1][columns]\n",
    "test = data[data[target] == -1][columns]\n",
    "tar = data[data[target] != -1][target]\n",
    "\n",
    "for col in object_col:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "    \n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2333)\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx], \n",
    "                           label=tar.iloc[trn_idx],\n",
    "                           categorical_feature=object_col)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx], \n",
    "                           label=tar.iloc[val_idx],\n",
    "                           categorical_feature=object_col)\n",
    "\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=2000, early_stopping_rounds = 200)\n",
    "    \n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "print(\"CV Score: {:<8.5f}\".format(r2_score(tar, oof_lgb))) \n",
    "\n",
    "eval_data = pd.read_csv('./data/评分文件/sub_a_913.csv')\n",
    "r2score = r2_score(eval_data['pre'],predictions_lgb)\n",
    "print(\"线上R^2 score:\",r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "       importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "       min_child_samples=20, min_child_weight=1, min_split_gain=0.0,\n",
       "       n_estimators=1000, n_jobs=-1, nthread=8, num_leaves=31,\n",
       "       objective='regression', random_state=None, reg_alpha=0.0,\n",
       "       reg_lambda=0.0, silent=True, subsample=0.8,\n",
       "       subsample_for_bin=200000, subsample_freq=0),\n",
       "       fit_params=None, iid='warn', n_jobs=10,\n",
       "       param_grid={'max_depth': range(5, 15, 2), 'learning_rate': [0.01, 0.05, 0.1], 'num_leaves': range(10, 40, 5)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_lgb = {\n",
    "    'max_depth': range(5,15,2),\n",
    "    'num_leaves': range(10,40,5),\n",
    "    'learning_rate':[0.01,0.05,0.1]\n",
    "}\n",
    "estimator = lgb.LGBMRegressor(\n",
    "    n_estimators = 1000, \n",
    "    objective = 'regression', \n",
    "    min_child_weight = 1, \n",
    "    subsample = 0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    nthread = 8,\n",
    ")\n",
    "gsearch = GridSearchCV(estimator , param_grid = params_lgb, scoring='r2', cv=3,n_jobs=10)\n",
    "gsearch.fit(train, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 11, 'learning_rate': 0.05, 'num_leaves': 20}\n",
      "0.8722676161713834\n",
      "0.9515421967765794\n"
     ]
    }
   ],
   "source": [
    "print(gsearch.best_params_)\n",
    "print(gsearch.best_score_)\n",
    "est = gsearch.best_estimator_\n",
    "eval_data = pd.read_csv('./data/评分文件/sub_a_913.csv')\n",
    "r2score = r2_score(eval_data['pre'],est.predict(test))\n",
    "print(r2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.7, eval_metric='rmse', gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=8, objective='reg:gamma', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=0,\n",
       "       subsample=0.8),\n",
       "       fit_params=None, iid='warn', n_jobs=10,\n",
       "       param_grid={'max_depth': range(5, 15, 2), 'eta': [0.01, 0.05, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in object_col:\n",
    "    train[col] = train[col].astype('float64')\n",
    "    test[col] = test[col].astype('float64')\n",
    "    \n",
    "params_xgb = {\n",
    "    'max_depth': range(5,15,2),  # 最大深度\n",
    "    'eta': [0.01,0.05,0.1],  # 学习率\n",
    "\n",
    "    }\n",
    "\n",
    "estimator = xgb.XGBRegressor(\n",
    "    n_estimators = 1000, \n",
    "    objective = 'reg:gamma', \n",
    "    min_child_weight = 1, \n",
    "    subsample = 0.8,\n",
    "    nthread = 8,\n",
    "    colsample_bytree = 0.7,\n",
    "    silent = 0,\n",
    "    eval_metric = 'rmse'\n",
    ")\n",
    "gsearch = GridSearchCV(estimator , param_grid = params_xgb, scoring='r2', cv=3, n_jobs=10)\n",
    "gsearch.fit(train, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'eta': 0.01}\n",
      "0.8634675175801168\n",
      "0.9457857720926782\n"
     ]
    }
   ],
   "source": [
    "print(gsearch.best_params_)\n",
    "print(gsearch.best_score_)\n",
    "est = gsearch.best_estimator_\n",
    "eval_data = pd.read_csv('./data/评分文件/sub_a_913.csv')\n",
    "r2score = r2_score(eval_data['pre'],est.predict(test))\n",
    "print(r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 真的是太慢了，甚至还没有开始指定的参数效果好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
